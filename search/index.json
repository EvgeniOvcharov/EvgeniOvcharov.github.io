[{"content":"Един от най-интригуващите аспекти в спортните състезания са противоречивите оценки, които фенове и състезатели дават за представянето на участниците. Отговорът на въпроса как да се определи най-добрият състезател или отбор е по-труден отколкото изглежда на пръв поглед. Това се дължи на факта, че освен уменията на участниците случайността също играе роля в определянето на спортните резултати. Казано по-просто в един спортен мач не винаги побеждава по-силният отбор. Към оформянето на спортните резултати, често се наслагва субективност в преценките на съдиите и организаторите на спортните събития. Как да се отстранят случайността и субективизма при оценяването на състезателите? За да се направи това е необходимо да се въведат безпристрастни критерии и алгоритми почиващи на здрави вероятностни и математически основи.\nПървите опити за оценяване на представянето на състезателите се базират на системи за натрупване на точки, или точкови системи за кратко. Оказва се обаче че точковите системи имат много слабости, когато се прилагат върху големи и сложни състезания. Те са по-подходящи за малки, затворени и елитни състезания, които осигуряват еднакви условия на всички участници. Такъв тип състезания са спортните лиги. В спортни лиги например е организиран Българският професионален клубен футбол. В националната „А“ група в момента играят 14 отбора в два полусезона всяка година. Във всеки полусезон всеки един от тези 14 отбора играе със останалите 13 свои противници точно един път. Така във всеки полусезон се играят $\\binom{14}{2} = 7 \\cdot 13 = 91$ мача. Всеки полусезон има 13 активни седмици, във всяка от които се играят 7 срещи. След края на втория полусезон се играят плейофи, където се определя шампиона за сезона. В края на всеки сезон два или три от най-слабо представените отбори отпадат от „А“ групата в по-долната „Б“ група, а от там идват същия брой най-добре представени отбори. През останалото време състезанието е затворено за нови участници.\nПример за истински масов спорт е шахът. По някои оценки между 300 млн и 1 млрд души играят шах любителски. Повечето държави имат национални федерации по шах, в които играят по-подготвени състезатели. Например,в Американската федерация по шах членуват 80 000 души. Във Световната федерация по шах ФИДЕ членуват около 360 000 души, но малък процент от членуващите в националните федерации членуват също във ФИДЕ. Може би не е случаен фактът, че именно при шаха възниква първата статистическа рейтингова система - системата Ело.\nОбщ преглед на системите за натрупване на точки Да започнем първо с някои дефиниции. Системите за натрупване на точки дават на състезателите определен брой точки според постигнатия резултат в даден мач и точките се натрупват с течение на времето. Една рангова система има за цел да подреди състезателите по резултатност в списък, който се нарича класация или ранг-листа и рангът на състезателя е неговото място в нея. Една рейтингова система има за цел да даде количествена оценка на някакво качество на състезателите, например тяхната спортна сила. Всяка рейтингова система може да се използва и като рангова система като стойностите на рейтингите определят мястото на състезателите в съответната ранг-листа. Например ФИФА и ФИДЕ съобщават за своите актуални ранг-листи, въпреки че те се създават на базата на рейтингови системи.\nВидове точкови системи Има два основни вида точкови системи. Първите отбелязват броя победи, а вторите размера на победите (margin of victory). Например в шаха се дава 1 точка за победа и 0 точки за загуба. В шаха е възможно да има равенство наричано реми, за което състезателите получават половин точка. Във футбола първоначално се е давало 2 точки за победа, 1 за равенство и 0 за загуба. Тези две точкови системи са еквивалентни, защото се различават единствено по постоянен множител. Сега във футбола се дават 3 точки за победа, а при останалите изходи точките са същите както преди. Това е направено с цел играта да стане по-оспорвана. В някои спортове мачът завършва с резултат, който автоматично определя размера на победата. Например това е головата разлика във футбола или разликата на отбелязаните точки в баскетбола. Ако тези разлики се натрупват за всеки отбор, ще получим точкова система от втория тип.\nТочковите системи от втори тип обаче рядко се използват на практика. Причината за това е че при тях има по-голямо изкушение да се наглася изхода на даден мач по нечестен начин. Също така те биха създали предпоставки силните отбори да надуват резултата колкото се може повече срещу слаби отбори. Това ще има нежелания ефект точно мачовете с надути резултати да имат най-силен принос към крайното класиране на отборите. Затова първият тип точкови системи са се наложили като стандартни. Трябва да се добави обаче, че когато се направят и сравнят статистически прогнозни модели които се базират на броя победи и на размера на победите, вторите имат по-добра точност. Това показва, че размерът на победите има допълнителна полезна информация над техния брой. Но не трябва да се забравя, че тези модели се прилагат върху данни от спортни състезания, които се организират с точкови системи от първия тип.\nНедостатъци на точковите системи Главният недостатък на точковите системи се изразява в това, че те не отчитат силата на противника и така победа над труден съперник и победа над лесен съперник носят едни и същи точки. В отворени и турнирни състезания, в които участниците често играят различен брой срещи се създават предпоставки да се избират лесни съперници, а силните противници да се избягват. Когато състезанието се състои от множество локални и автономни турнири възниква необходимостта да се оценя важността и престижа на всеки такъв турнир поотделно. Преценката колко точки един турнир да носи на своите участници има субективен характер.\nВъв ФИФА например членуват 211 национални футболни асоциации, които се организирани в 6 континентални конфедерации. Освен Световното първенство по футбол (the World Cup), под егидата на ФИФА се организират огромен брой футболни турнири, квалификационни и приятелски мачове. Пример за регионален турнир е Балканската купа провеждан от 1929 и до 1980 г. между държавите от Балканския полуостров.\nДо финалите на Световната купа през 2018 ФИФА използва точкова система, след които е заменена със статистическата рейтингова система Ело. Причината за тази замяна стават честите аномалии, до които точковата система е водила в рангирането на отборите. Като пример ще разгледаме следните два случая. През 2014 Бразилия изпада до рекордно ниското 22 място в ранг-листата на ФИФА точно преди началото на своето домакинство на Световната купа. Там тя се класира на 4-то място. През 2018 година домакин на Световната купа е Русия. Точно преди началото на шампионата тя се намира на 70-то място в ранг-листата на ФИФА, но достига до четвъртфинали, където е отстранена след продължения и дузпи. Причината за тези несъответствия се е дължала на автоматичното класиране на турнира на домакините. Така предходните две години те не играят квалификационни мачове, които носят много точки, а само приятелски мачове, които носят по-малко точки. Този пример разкрива една от слабостите на точковото оценяване. При големи и сложни състезания често възникват непредвидени ситуации, за които точковите системи не са добре балансирани. За разлика от тях, рейтинговите системи са по-лесни за балансиране, защото те зависят от малък на брой математически правила.\nНякои типове рейтингови системи Рейтингови системи обобщаващи точкови системи Сега ще разгледаме една рейтингова система, която се извежда чрез метода на най-малките квадрати. Самото извеждане ще разглеждаме по-късно, а сега ще се спрем на самия резултат. Означаваме чрез $\\theta_1, \\theta_2, \\dots, \\theta_n$ рейтингите на n състезателя или отбора. За конкретност ще говорим за отбори и ще разгледаме само спортове от рода на футбола, в които крайният резултат представлява отбелязани голове или точки. Отбелязваме с ${\\operatorname{pd}}_1, {\\operatorname{pd}}_2, \\cdots, {\\operatorname{pd_n}}$ техните сумарни голови (точкови) разлики от всички играни мачове от началото на състезанието до момента на определяне на рейтингите. Съкращението pd идва от английското point difference. Стойностите на рейтингите се определят от следната линейна система n от уравнения:\n$$\\begin{aligned} \\label{eq: lsq rat} \\theta_i = \\frac 1 {n_i} \\sum_{i \\not= j} n_{ij}\\theta_j + \\frac 1 {n_i} {\\operatorname{pd_i}}, \\quad i=1,\\dots,n,\\end{aligned}$$\nкъдето $n_i$ е броят мачове, които $i$-тия отбор е изиграл, а $n_{ij}$ е броят мачове, които $i$ и $j$ са изиграли заедно. Разбира се имаме, че $n_i = \\sum_{i \\not=j} n_{ij}$. Членът $$\\frac 1 {n_i} \\sum_{i \\not= j} n_{ij}\\theta_j$$ дава средният рейтинг на опонентите на отбор $i$, а членът $$\\frac 1 {n_i} {\\operatorname{pd_i}}$$ дава средния размер на победите (средната голова разлика) на отбор $i$. Той отговаря на „чиста“ точкова система. Да запишем системата в матричен вид. Полагаме $D = (n_1, n_2, \\dots, n_n)$, $A =(n_{ij})_{i,j=1}^n$, $b = ({\\operatorname{pd_1}}, {\\operatorname{pd_2}}, \\dots, {\\operatorname{pd_n}})$, $\\theta = (\\theta_1, \\theta_2, \\dots, \\theta_n)$ и получаваме $$D\\theta = A\\theta + b.$$ Забележете, че $A$ е матрицата на съседство на претеглен граф, в който теглата на ребрата отговарят на броя мачове, които съответните отбори са играли. Системата може да бъде записана още така\n$$\\theta = P\\theta + \\beta,$$ където $P = D^{-1}A$ е стохастична матрица по редове, а $\\beta = D^{-1}b$ дава средния размер на победите (средните голови разлики) на отборите. Тъй като $P$ е стохастична матрица по редове, векторът $e = c(1,1,\\dots, 1)$ е собствен вектор на $P$ отговарящ на собственото число 1. Следователно матрицата $I-P$ има детерминанта 0, тоест тя е изродена, и системата по-горе има или нула решения или безкрайно много решения. Може да се докаже, че винаги има безброй много решения, защото векторът $\\beta$ също има специална структура. Например за него винаги е изпълнено равенството $${\\operatorname{pd}}_1 + {\\operatorname{pd}}_2 + \\cdots + {\\operatorname{pd}}_n = 0.$$ Ако матрицата на съседство $A$ отговаря на свързан граф, то $\\operatorname{rank}{A} = n-1$, иначе $\\operatorname{rank}{A} = n-r$, където $r$ е броят на свързани компоненти на социалния граф на състезанието. Подобна на горната зависимост ще възникне между елементите $\\beta$ отговарящи на всеки свързан компонент на $A$.\nДа приемем сега, че $A$ има само един свързан компонент. Тогава можем да получим единствено решение на , ако наложим на системата едно линейно условие. Как да изберем подходящо условие? За целта нека да съобразим, че ако $\\theta$ е решение на , то $\\theta- \\alpha e$ е също решение, където $\\alpha$ е произволна реална константа. Тоест ние можем да изместваме всички рейтинги чрез произволна транслация и това, което е важно е само относителните разлики между тях. Затова избираме следното условие $$\\begin{aligned} \\label{eq: sum 0} \\theta_1 + \\theta_2 + \\cdots + \\theta_n = 0.\\end{aligned}$$ С това условие положителен рейтинг означава, че съответния отбор се представя по-добре от средния отбор в състезанието, а отрицателен рейтинг означава обратното.\nСега ще покажем в какъв смисъл е обобщение на точкова система. Да приемем, че състезанието е организирано като спортна лига, в която всеки отбор играе срещу всеки свой съперник точно 2 пъти. Тогава имаме $n_i = 2(n-1)$ за всяко $i$ и уравнението се свежда до $$(n-1) \\theta_i = \\theta_1 + \\theta_2 + \\cdots + \\theta_n - \\theta_i + {\\operatorname{pd}}_i / 2$$ или $$\\theta_i = \\frac{1}{2n} = {\\operatorname{pd}}_i,$$ взимайки предвид . Така всички рейтинги се свеждат до константа по сумарната головата разлика на отборите, тоест са еквивалентни на система за натрупване на точки.\nЗакон на Thurstone за сравнителните съждения На английски, Thurstone’s Law of Comparative Judgement е публикуван в статия на автора от 1927 година. В психологията съждение е оценка или преценка, на анг. judgement. Законът за сравнителни съждения (ЗСС) е модел описващ как силата на външни стимули се представя от съзнанието на човека като точки от вътрешна, психична скала. Тези стимули може да имат физическо измерение като звук или тежест или чисто психично измерение като нашето отношение към някаква съвкупност от обекти или абстрактни идеи. ЗСС служи като теоретична рамка, на базата на която могат да се построи важен клас от рейтингови системи в спорта. Сега ще представим основните идеи на този закон.\nДа вземем например външен стимул като интензитета на звука. Като физическо проявление, той се измерва от учените във физична скала в мерни единици децибели. Но в нашето съзнание той се регистрира във друга, нефизична скала на възприятия. Нашите рецептори регистрират звука и съзнанието стартира процес на разпознаване на силата на звука като точка от тази вътрешна скала. Този процес е съпроводен със случайни флуктуации поради грешки в човешките рецептори или неврони. Затова представянето на този стимул във вътрешната скала е случайна величина, която най-често се приема да е нормално разпределена и има вида $$\\begin{aligned} \\label{eq: S} S = \\theta + \\epsilon, \\quad \\epsilon \\in N(0, \\sigma^2).\\end{aligned}$$\nПараметърът $\\theta$ се нарича модалната стойност на процеса на разпознаване на външния стимул. Това е най-вероятната точка от вътрешната скала, с която този стимул се представя. Параметърът $\\sigma$ дава стандартното отклонение на този процес на разпознаване. Ние обаче нямаме директен достъп до стойностите от тази вътрешна скала. Това, което можем да направим е да сравняваме стимулите по големина. Тоест, ако имаме два (вътрешно представени) стимула $S_i$ и $S_j$ ние можем да определяме дали $S_i - S_j \u0026gt; 0$ или тази разлика е отрицателна или равна на нула.\nСега ще илюстрираме ЗСС в контекста на спортните съревнования. Това, което искаме да оценим е спортната сила на отборите. Тя обаче лежи във вътрешна, скрита скала и ние можем да съдим за нея само индиректно, по спортните резултати на отделните мачове. Допускаме, че представянето $S$ на един отбор по време на игра е нормално разпределено около своята модална стойност $\\theta$ по закона даден в . Модалната стойност $\\theta$ представлява спортната сила на съответния отбор, чиято стойност ние искаме да отъждествим с рейтинга на този отбор. Да разгледаме сега представянията на два отбора\n$$\\begin{aligned} S_i \u0026amp;= \\theta_i + \\epsilon_i, \\quad \\epsilon_i \\in N(0, \\sigma_i^2)\\\nS_j \u0026amp;= \\theta_j + \\epsilon_j, \\quad \\epsilon_j \\in N(0, \\sigma_j^2).\\end{aligned}$$\nКакто казахме $S_i$ и $S_j$ не са директно наблюдаеми, те съвпадат с нашето възприятие за силата на представяне на двата отбора. Това, което можем да направим винаги е да отсъдим кое от двете представяния е по-високо в нашето съзнание. В спорта обаче обикновено не се разчита на субективната преценка на съдии или наблюдатели да се решава този въпрос. Всеки спортен двубой завършва с резултат и отборът с по-висок резултат се приема, че се е представил по-добре от своя съперник.\nНека сега построим вероятностен модел за това един отбор да победи друг отбор. Изпълнено е\n$$\\begin{aligned} \\begin{split}\\label{eq: paired comp} \\Pr(S_i -S_j \u0026gt; 0 ) \u0026amp;= \\Pr(\\epsilon_i - \\epsilon_j \u0026gt; - (\\theta_i - \\theta_j) ) \\\n\u0026amp;= \\Phi\\left( \\frac{\\theta_i - \\theta_j}{\\sqrt{\\sigma_i^2 + \\sigma_j^2}} \\right), \\end{split}\\end{aligned}$$\nкъдето $\\Phi$ е кумулативната функция на разпределението на нормалното разпределение. За да изведем последното равенство сме допуснали, че случайните грешки $\\epsilon_i$ и $\\epsilon_j$ са независим. Ако това не е така, пак ще получим подобно равенство, в което участва и коефициента на корелация между $\\epsilon_i$ и $\\epsilon_j$.\nВероятностният модел от вида се нарича модел за сравнение по двойки. Най-важните частни случаи носят имената на известни математици. Например, моделът , където $\\Phi$ е КФР на нормалното разпределение се нарича модел на Thurstone-Mosteller. Ако $\\Phi$ е КФР на логистичното разпределение, то този модел носи имената на Bradley-Terry. Всеки един от тези два модела е база на известната рейтингова система Ело, която ще разглеждаме по-късно. Само ще споменем, че Ело използва заедно с метода на стохастичния градиент (stochastic gradient descent) от стохастичното оптимиране да изчислява рейтингите $\\theta$.\n","date":"2021-03-02","permalink":"/post/lection-1/","tags":null,"title":"Лекция 1  Защо има нужда от рейтингови системи в спорта?"},{"content":"Comma-separated value (csv) files are one of the most common file formats used in data analysis today. Sometimes we need to read multiple csv files from disk and combine them into a single data frame or data table object in R. We shall explore five different approaches to that task and determine the most efficient one. First, let us make sure that we know how to answer the following question:\nHow to list the files in a given directory? The function list.files() lists all files in a given directory whose names contain a specific character pattern. An example of its use can be the following:\nlist.files(path = \u0026quot;./csv/\u0026quot;, pattern = \u0026quot;^f.*199\u0026quot;, full.names = TRUE)\r[1] \u0026quot;./csv/football-results-1998.csv\u0026quot; \u0026quot;./csv/football-results-1999.csv\u0026quot;\r The output is a character vector giving the names of the files matching the search criterion. In our example, we have called the function with the following options:\n  path = \u0026quot;./csv/\u0026quot; the directory path is set to the csv subdirectory of the working directory;\n  pattern = \u0026quot;^f.*199\u0026quot; a regex expression matching strings starting with the character f, followed by an unspecified number of arbitrary characters, followed by 199;\n  full.names = TRUE the path is attached to the returned file names.\n  Examining five approaches The stated problem is different from looking for the fastest way to read one large csv file into memory. In our use case we need to import multiple csv files (say 10 or more), each having many columns (say 10 or more), which we need to row bind together to produce a single large data frame or data table object. We next list five approaches to that task in increasing order of efficiency.\nThe variable files in the code below is a character vector containing a list of names of csv files, for example as produced by list.files().\n1. Sequentially reading and binding all data frames table = NULL\rfor (file in files)\rtable = rbind(table, read.csv(file))\r This is the least efficient solution in which we simply append each data frame sequentially, starting with an empty data.frame object. At each step R stores the intermediate data frame at some new address in memory, which results in copying the data of the earlier data frames multiple times.\n2. Making a single call to rbind()  table = lapply(files, read.csv) %\u0026gt;% do.call(rbind, .)\r Here we use the fact that the function rbind() can simultaneously bind together multiple data frames, which is significantly faster than the previous approach. A special feature of R functions like rbind() is having a dynamic argument list. The R way of passing a variable number of arguments to a function is by invoking the method do.call().\n3. Further optimization with rbindlist() table = lapply(files, read.csv) %\u0026gt;% rbindlist()\r The row binding of multiple data frames and other matrix-like structures with the same column names can be optimized with the function rbindlist(), a member of the data.table package.\n4. A solution based on the newer readr package col_spec = spec_csv(files[1])\rtable = lapply(files, read_csv, col_types = col_spec) %\u0026gt;% rbindlist()\r Another optimization may come from the replacement of read.csv() with a faster analogue of it. Such is the function read_csv() from the package readr. The function read_csv(), however, may be very slow and inefficient if it has to determine the column types of the data very often. To overcome this problem we have first invoked readr::spec_csv() to automatically set up the column types before running the rest of the code.\n5. A solution based on the data.table package table = lapply(files, fread) %\u0026gt;% rbindlist()\r The package data.table is a modern and much optimized analogue to the more familiar package data.frame. The function fread() from data.table is designed to read a csv file into a data.table object. For large csv files it can be an order of magnitude faster than read.csv().\nComparison of running times We have benchmarked the relative efficiency of each approach in the table below.\n\r\rtest \rreplications \relapsed \ruser \rsystem \r\r\r\r\rfread \r100 \r3.37 \r2.84 \r0.53 \r\r\rread_csv \r100 \r9.02 \r5.83 \r2.81 \r\r\rrbindlist \r100 \r16.51 \r16.09 \r0.42 \r\r\rdo.call \r100 \r19.48 \r19.06 \r0.41 \r\r\rsequential \r100 \r36.83 \r36.26 \r0.50 \r\r\r\rApart from these results, we also note the following:\n  The test involves reading 22 csv files with 15 columns (a mixture of character and integer values) and around 1000 rows each with actual data on international football matches. The elapsed time in the table above is given in seconds for 100 repetitions.\n  The relative performance of the functions fread(), read_csv() and read.csv() under equal conditions may be gleaned from the first three results in the table above.\n  The method rbindlist() is faster than rbind() called with do.call() for multiple data.frame elements, but not for data.table ones. So, in fact replacing Solution 5 above with the code table = lapply(files, fread) %\u0026gt;% do.call(rbind, .) results in marginally faster running times.\n  Without specifying the column types in read_csv() the function runs very slowly and we get a 4-5 fold reduction in speed. Its default usage, such as in lapply(files, read_csv) is not recommended in a set-up like ours.\n  ","date":"2019-08-24","permalink":"/post/2019-08-21-reading-csv-files/","tags":["Programming in R"],"title":"Reading multiple csv files in R"},{"content":"Let our probability space $(\\Omega, \\mathcal B, \\lambda)$ be the unit interval $\\Omega=[0,1]$ with the Borel subsets $\\mathcal B$ and the Lebesgue measure $\\lambda$. Is it possible to find an infinite sequence of independent and identically distributed random variables $X_1$, $X_2$, $\\dots$ of any given distribution supported on this probability space?\nSurprisingly, yes, and the proof relies on a version of Cantor\u0026rsquo;s diagonal argument.\nProof It is sufficient to find a sequence of independent and uniformly distributed random variables $U_1$, $U_2$, $\\dots$, on $[0,1]$. Then if $F$ is any cumulative distribution function, the sequence $X_n = F^{-1}(U_n)$ has the desired property. Thus we proceed to construct the required sequence $U_1$, $U_2$, $\\dots$\nIn order to apply Cantor\u0026rsquo;s diagonal argument, we first need to write an arbitrary point $a \\in [0,1]$ as an infinite binary fraction\n$$ \\begin{aligned} a = 0.\\omega_1\\omega_2\\omega_3\\dots, \\quad \\omega_i \\in {0,1}. \\end{aligned} $$\nIntuitively, we need to partition the information about $a$ into an infinite collection of portions and use each portion to construct a new random variable.\nWe urge the reader to try finishing the proof on his or her own. Here is my approach.\nApplying Cantor’s diagonal argument So let $\\mathcal A_1$ be an infinite subsequence of $\\mathcal A = \\{ \\omega_1, \\omega_2, \\omega_3, \\dots \\}$ such that the remaining subsequence of binary digits is also infinite. We select another infinite subsequence $\\mathcal A_2$ of the remainder with the same condition. We may continue this process ad infinitum and obtain an infinite collection of subsequences $\\mathcal A_i$. To each subsequence $\\mathcal A_i$ we correspond a real number $a_i\\in[0,1]$ with binary expansion given by the subsequence. We finish this construction by taking $U_i = a_i$.\nLet us now show that each $U_i$ is uniform. This follows from the fact that the value of $U_i$ is a binary fraction in $[0,1]$ whose digits have been drawn independently from the set $\\{0,1\\}$. Clearly, this process produces an arbitrary number on $[0,1]$. The variables $U_i$ are also independent because they do not contain any information about each other.\nRemark Although aesthetically satisfying, Cantor\u0026rsquo;s diagonal argument brought us to a conclusion which clashes deeply with our intuition. Indeed, it allowed us to effectively clone a single random variable into an infinite sequence of independent random variables. We are reminded once more that real numbers are collections of infinite information, which is something we tend to forget when we associate finite data with real numbers. And one infinite collection contains as much information as an infinity of infinite collections. The argument would have failed on probability spaces containing only rational points for example. Nevertheless, such arguments have their use in mathematics as sometimes they demonstrate a negative result - an impossibility of a certain property. In other cases like this one, they demonstrate a positive result, which cannot be observed directly, but only in terms of finite approximations.\n","date":"2018-08-28","permalink":"/post/2018-08-28-independence-and-diagonalization/","tags":["Probability"],"title":"Independence and Cantor's Diagonal Argument"},{"content":"Here we illustrate three of the famous zero-one laws for convergent sequences of random variables. Their names are: the Borel-Cantelli lemma, the second Borel-Cantelli lemma, and Kolmogorov\u0026rsquo;s zero-one law. They are going to help us study the nuances in the relationship between almost sure convergence and complete convergence.\n Suppose we are given an infinite sequence of random variables\n$$ \\tag{1}{X_1, X_2, \\dots} $$\ndefined on some probability space $(\\Omega, \\mathcal A, \\operatorname P)$. We list three different modes in which the sequence may be convergent to zero.\n $(i)$ The sequence $(1)$ converges to zero in probability if for every $\\epsilon\u0026gt;0$  $$ \\lim_{n\\rightarrow \\infty} \\operatorname P(|X_n|\u0026gt;\\epsilon) = 0. $$\n $(ii)$ The sequence $(1)$ converges to zero almost surely if for every $\\epsilon\u0026gt;0$  $$ \\lim_{n\\rightarrow \\infty} \\operatorname P({|X_n|\u0026gt; \\epsilon} \\cup {|X_{n+1}|\u0026gt; \\epsilon} \\cup \\cdots) = 0. $$\nIt is easily seen that this is equivalent to $\\operatorname P \\left(\\limsup_{n\\to \\infty }E_{n}\\right)=0,$ which is equivalent to the usual condition $\\operatorname P(\\lim_{n\\rightarrow\\infty} X_n = 0)=1$.\n $(iii)$ The sequence $(1))$ converges to zero completely if for every $\\epsilon\u0026gt;0$  $$ \\lim_{n\\rightarrow \\infty} \\operatorname P({|X_n|\u0026gt; \\epsilon}) + \\operatorname P({|X_{n+1}|\u0026gt; \\epsilon}) + \\cdots = 0. $$\nClearly, $(iii)$ implies $(ii)$ and $(i)$, and $(ii)$ implies $(i)$. The example: $\\Omega = [0,1]$, $\\operatorname P$ is the Lebesgue measure, $X_n = 1$ for $0\u0026lt;\\omega \u0026lt; 1/n$, $X_n=0$ otherwise, shows that $(ii)$ does not imply $(iii)$.\nWe are going to show that $(ii)$ and $(iii)$ are equivalent if $X_n$ are independent. To that end, we need to revise some facts.\nBorel-Cantelli lemma  Let $E_1$, $E_2$,\u0026hellip; be a sequence of events in some probability space. If the sum of the probabilities of $E_n$ is finite\n$$ \\sum_{n=1}^{\\infty } \\operatorname P(E_{n})\u0026lt;\\infty , $$\nthen the probability that infinitely many of them occur is $0$, that is,\n$$ \\operatorname P \\left(\\limsup_{n\\to \\infty }E_{n}\\right)=0. $$\n The Borel-Cantelli lemma is an almost obvious fact which can be thought of as a more precise way of stating that complete convergence implies almost sure convergence, that is, that $(iii)$ implies $(ii)$. There is a partial converse to it, which is more useful to our goals.\nSecond Borel-Cantelli lemma  If the events $E_n$ are independent and the sum of the probabilities of $E_n$ diverges to infinity,\n$$ \\sum_{n=1}^{\\infty} \\operatorname P(E_{n})=\\infty, $$\nthen the probability that infinitely many of them occur is $1$,\n$$ \\operatorname P \\left(\\limsup_{n\\rightarrow \\infty } E_{n} \\right) = 1. $$\n Denoting by $E_n$ the events $E_n = \\{ \\mid X_n \\mid \u0026gt; \\epsilon \\}$, we have that they are independent under the assumption that $X_n$ are independent. If, in addition, $(ii)$ holds, but $(iii)$ does not, we reach to a contradiction in view of the second Borel-Cantelli lemma. Thus, under independence the latter two modes of convergences are equivalent.\nThe last thing we would like to touch upon here is the limit of a sequence of independent random variables. Such a limit may only be a constant, but in order to show this we need another fundamental convergence result.\nKolmogorov\u0026rsquo;s zero–one law  Kolmogorov\u0026rsquo;s zero–one law specifies that a certain type of event, called a tail event, will either almost surely happen or almost surely not happen; that is, the probability of such an event occurring is zero or one. Tail events are defined in terms of infinite sequences of random variables. Suppose $X_1, X_2, \\dots$ is an infinite sequence of independent random variables (not necessarily identically distributed). Let $\\mathcal {F}$ be the $\\sigma$-algebra generated by all $X_{i}$ in the sequence. Then, a tail event $F\\in {\\mathcal {F}}$ is an event which is probabilistically independent of each finite subset of these random variables. Note that $F$ belonging to $\\mathcal {F}$ implies that whether $F$ occurs or not is uniquely determined by the values of all $X_{i}$.\nFor example, the event that the sequence converges and the event that its sum converges are both tail events. In an infinite sequence of coin-tosses, a sequence of 100 consecutive heads occurring infinitely many times is a tail event. Intuitively, tail events are precisely those events whose occurrence can still be determined if an arbitrarily large but finite initial segment of the $X_{i}$ are removed. In many situations, it can be easy to apply Kolmogorov\u0026rsquo;s zero–one law to show that some event has probability 0 or 1, but surprisingly hard to determine which of these two extreme values is the correct one.\nExact formulation of Kolmogorov\u0026rsquo;s zero–one law  Let $(\\Omega, \\mathcal A, \\operatorname P)$ be a probability space and let $\\mathcal F_n$ be a sequence of mutually independent $\\sigma$-algebras contained in $\\mathcal F_n$. Let\n$$ \\mathcal G_{n}=\\sigma {\\bigg (}\\bigcup_{k=n}^{\\infty }\\mathcal F_{k}{\\bigg )} $$\nbe the smallest $\\sigma$-algebra containing $\\mathcal F_n, \\mathcal F_{n+1}, \\dots$ Then for any event\n$$ F \\in \\bigcap_{n=1}^{\\infty } \\mathcal G_{n} $$\none has either $\\operatorname P(F) = 0$ or $1$.\n We now apply Kolmogorov\u0026rsquo;s zero–one law to show that the limit $X$ of a sequence of independent random variables $X_n$ must be a constant. The argument is valid for multivariate random variables of arbitrary dimension $d$. We begin by noticing that any set of the form $\\{ X \\in A \\mid A \\in \\mathcal B(\\operatorname R^d)\\}$ represents a tail event. Therefore, there must be a sufficiently large hypercube $Q_0$ with sides parallel to the coordinate axes and center at the origin such that $\\operatorname P(X \\in Q_0) = 1.$ Dividing each side by two, we partition $Q_0$ into $2^d$ smaller hypercubes. In view of Kolmogorov\u0026rsquo;s zero–one law, there must be exactly one of these hypercubes, denoted by $Q_1$, such that $\\operatorname P(X \\in Q_1) = 1$. We continue this process iteratively and obtain an infinite sequence $Q_0, Q_1, \\dots$ of nested hypercubes. Following Cantor\u0026rsquo;s argument, the intersection of all these hypercubes is a single point, $c$, and it follows that $\\operatorname P(X = c) = 1$. This concludes the proof of the claim that $X$ must be a constant.\n","date":"2018-08-27","permalink":"/post/2018-08-24-complete-convergence/","tags":["Probability"],"title":"Complete Convergence and the Zero-One Laws"},{"content":"Problem Compute the integral\n$$ I = \\int_0^\\infty \\frac 1 {1+x^4} dx. $$\nAnswer  This seems to be a popular problem illustrating various mathematical techniques. The standard approach is to use complex analysis and the Cauchy method of residues. The problem may be solved with more elementary techniques by using a series of clever substitutions. Both approaches rely on quite lengthy computations to obtain the answer\n$$ I = \\pi \\frac {\\sqrt 2} 4. $$\nHere, we would like to show that the answer may be obtained quite easily in the form of power series.\nPower series solution We write\n$$ I = \\int_0^1 \\frac 1 {1+x^4} dx + \\int_1^\\infty \\frac 1 {1+x^4} dx = I_1 + I_2. $$\nFor the first integral we have\n$$ I_1 = \\int_0^1 \\sum_{k=0}^\\infty (-1)^k x^{4k} dx = \\sum_{k=0}^\\infty (-1)^k \\frac 1 {4k + 1}. $$\nFor the second integral we make the substitution $x = 1/y$ and get\n$$ I_2 = \\int_0^1 \\frac {y^2} {1+y^4} dy = \\int_0^1 \\sum_{k=0}^\\infty (-1)^k y^{4k + 2} dy = \\sum_{k=0}^\\infty (-1)^k \\frac 1 {4k + 3}. $$\nSo the final result is\n$$ I = \\sum_{k=0}^\\infty (-1)^k \\frac 1 {4k + 1} + \\sum_{k=0}^\\infty (-1)^k \\frac 1 {4k + 3}. $$\nFor example, if we take the first few members of each series we obtain the approximation\n$$ I \\approx \\left(1 - \\frac 1 5 + \\frac 1 9 - \\frac 1 {13} + \\frac 1 {17}\\right) + \\left(\\frac 1 3 - \\frac 1 7 + \\frac 1 {11} - \\frac 1 {15}\\right) \\approx 1.1077, $$\nwhose error is less than $0.003$ of the exact value of $I$.\nIt would be an interesting exercise to find the sums of the above power series in closed form.\n","date":"2018-08-23","permalink":"/post/2018-08-18-computing-integral/","tags":["Mathematical Analysis"],"title":"Computing an Integral"},{"content":"Here we consider the basic asymptotic properties of the coefficient estimators in a simple linear regression. Many readers when first acquainted with the subject stay under the expression that this is a rather tedious and technical matter. In what follows, we would like to show that when approached from the right perspective, the subject becomes rather intuitive and clear.\nDerivation of the estimators We consider a simple linear regression model\n$$ Y = \\beta_0 + \\beta_1 X + \\epsilon, $$\nwhere $X$ and $Y$ are random variables, $\\beta_0$, $\\beta_1$ are constants and $\\epsilon$ is the random error. We assume that the observations $(x_1,y_1), \\dots, (x_n,y_n)$ are independently and identically distributed. In the ordinary least squares (OLS) method, we want to minimize the mean squared error\n$$ \\tag{1} \\min J(b_0, b_1) = \\frac 1 n \\sum_i (y_i - b_0 - b_1 x_i)^2 $$ in terms of the unknown constants $b_0$, $b_1$. Solving\n$$ \\frac{\\partial J(b_0, b_1)}{\\partial b_0} = \\frac 2 n \\sum_i (y_i - b_0 - b_1 x_i) = 0 $$\nfor $b_0$, we obtain $\\hat \\beta_0 = \\bar y - b_1 \\bar x$. After finding the optimal value for $b_1$, which we denote by $\\hat \\beta_1$, we will get the relation $\\bar y = \\hat \\beta_0 + \\hat \\beta_1\\bar x$. This means that the regression line always passes through the point $(\\bar x, \\bar y)$. Since the objective function $J$ is convex, its critical points are minima. That is why we may substitute $\\hat \\beta_0 = \\bar y - b_1 \\bar x$ in the expression for $J(b_0, b_1)$ in $(1)$ and eliminate $b_0$\n$$ J(b_0, b_1) = J(b_1) = \\frac 1 n \\sum_i ((y_i - \\bar y) - b_1 (x_i-\\bar x))^2. $$\nComparing the above with $(1)$, we notice that centering the data leads to a regression model with zero free term, that is, to a regression line through the origin. (Of course for the original, uncentered data $\\hat\\beta_0$ would typically not be zero.) Solving for $b_1$ in\n$$ \\frac{\\partial J(b_1)}{\\partial b_1} = \\frac 2 n \\sum_i (x_i-\\bar x)((y_i - \\bar y) - b_1 (x_i-\\bar x)) = 0 $$\nwe obtain that\n$$ \\hat \\beta_1 = \\frac {c_{xy}}{d_x} = \\frac{\\frac 1 n \\sum_i (x_i-\\bar x)(y_i - \\bar y)}{\\frac 1 n \\sum_i (x_i-\\bar x)(x_i - \\bar x)}. $$\nNotice that $\\hat \\beta_1$ is the ratio of the sample covariance between $X$ and $Y$ and the sample variance of $X$.\nAsymptotic properties Here, we address the questions whether the estimators $\\hat \\beta_0$ and $\\hat \\beta_1$ are unbiased and consistent. We first treat the case of the $\\hat \\beta_1$ estimator. The trick is to write it in the form\n$$ \\hat \\beta_1 = \\beta_1 + \\text{\u0026ldquo;an error term\u0026rdquo;}. $$\nIn the expression of $c_{xy}$ we substitute $y_i$ with the model relation $y_i - \\bar y = \\beta_1(x_i - \\bar x) + \\epsilon_i$. We arrive immediately at\n$$ \\tag{2} \\hat \\beta_1 = \\beta_1 + \\frac{\\sum_i (x_i-\\bar x)(\\epsilon_i - \\bar \\epsilon)}{\\sum_i (x_i-\\bar x)(x_i - \\bar x)} = \\beta_1 + \\frac {c_{x\\epsilon}}{d_x}, $$\nwhere $c_{x\\epsilon}$ denotes the sample covariance between $(x_1,\\dots, x_n)$ and $(\\epsilon_1,\\dots,\\epsilon_n)$. Notice that since $\\sum_i (x_i-\\bar x)\\bar \\epsilon = 0$ we may freely add or subtract this term to the above expression.\nThe standard assumptions for the error terms $\\epsilon_i$ are that they are mutually independent and also $\\operatorname E[\\epsilon_i] = 0$, which imposes no real restriction as the mean error may always be subsumed in the free term $\\beta_0$. In connection to the predictors, some models make the quite restrictive assumption of independence between the error terms and the predictors. Here we are going to assume the weaker condition $\\operatorname E[\\epsilon_i \\mid X] = 0$ instead. As an immediate consequence of the latter we have that\n$$ \\operatorname E[\\epsilon_i] = \\operatorname E [\\operatorname E[\\epsilon_i \\mid X]] = 0 $$\nand also that $X$ and $\\epsilon$ are uncorrelated. Indeed,\n$$ \\operatorname{cov}(X,\\epsilon) = \\operatorname E[X\\epsilon] - \\operatorname E[X]\\operatorname E[\\epsilon] = \\operatorname E \\operatorname E[X\\epsilon \\mid X]] - 0 = 0. $$\nWe may write $\\hat \\beta_1$ in the form\n$$ \\hat \\beta_1 = \\beta_1 + \\sum_i \\phi(x_1,\\dots, x_n )\\epsilon_i. $$\nUsing the fact that $\\operatorname E[\\epsilon_i \\mid x_1,\\dots, x_n] = 0$, we find that $\\hat \\beta_1$ is unbiased\n$$ \\operatorname E[\\hat \\beta_1] = \\beta_1 + \\operatorname E [\\phi(x_1,\\dots, x_n )\\operatorname E[\\epsilon_i \\mid x_1,\\dots, x_n]] = \\beta_1. $$\nFor $\\hat \\beta_0 = \\bar y - \\hat \\beta_1 \\bar x$, we have\n$$ \\operatorname E[\\hat \\beta_0] = \\operatorname E[\\bar y] - \\operatorname E [\\operatorname E[\\hat \\beta_1 \\bar x\\mid x_1,\\dots, x_n]] = \\operatorname E[Y] - \\beta_1 \\operatorname E[X] = \\beta_0. $$\nThus $\\hat \\beta_0$ is also an unbiased estimator.\nBy the law of large numbers $c_{x\\epsilon} \\rightarrow \\operatorname{cov}(X,\\epsilon)=0$ and $d_x \\rightarrow \\operatorname D[X]$, the variance of $X$. In view of $(2)$, we conclude that $\\hat \\beta_1 \\rightarrow \\beta_1$ is a consistent estimator. Similarly,\n$$ \\hat \\beta_0 = \\bar y - \\hat \\beta_1 \\bar x \\rightarrow \\operatorname E[Y] - \\beta_1 \\operatorname E[X] = \\beta_0 $$\nis also a consistent estimator.\nNumerical example Here, we are going to verify the law of large numbers empirically. We shall check whether the sample covariance converges to the population covariance as the number of trials tends to infinity. Let us consider $Y \\sim N(0, X^2)$, $X \\sim U(0,1)$. We have that $\\operatorname E[Y\\mid X] =0$ and from the law of total expectation $\\operatorname E[Y] = 0$ and $\\operatorname E[XY] = 0$. Hence, $\\operatorname{cov}(X,Y)= \\operatorname E[XY] - \\operatorname E[X]\\operatorname E[Y] = 0$, but $X$ and $Y$ are not independent as the knowledge of $X$ alters the probability distribution of $Y$.\nBelow is a code in R which produces the numerical experiment. Here $m_{xy}$ is the sample mean of $XY$, $c_{xy}$ is the sample covariance of $X$ and $Y$, $d_x = c_{xx}$ is the sample variance of $X$, and $d_y = c_{yy}$ is the sample variance of $Y$.\nm_xy = numeric(); c_xy = numeric(); d_x = numeric(); d_y = numeric();\rsize = c(10, 100, 1000, 10000, 100000)\rfor(n in size){\rx = runif(n); y = rnorm(n, 0, x*x);\rm = mean(x*y); m_xy = c(m_xy, m);\rc_xy = c(c_xy, m - mean(x)*mean(y))\rd_x = c(d_x, var(x)); d_y = c(d_y, var(y))\r}\rdata = data.frame(m_xy = m_xy, c_xy = c_xy, d_x = d_x, d_y = d_y, row.names = size)\r The results are shown in the table below.\n\r   $n$ $m_{xy}$ $c_{xy}$ $d_x$ $d_y$     10 -0.0220692 -0.0130559 0.0931474 0.1335045   100 0.0177529 0.0153785 0.0878564 0.1566800   1000 -0.0008616 0.0005846 0.0819234 0.1919942   10000 -0.0047650 -0.0015213 0.0832527 0.2027653   100000 0.0008292 0.0004920 0.0833878 0.1992950    \r\rtable {\rwidth:100%;\r}\r\r","date":"2018-08-16","permalink":"/post/2018-08-16-regression-coefficients/","tags":["Regression"],"title":"Unbiasedness and Consistency of the Regression Coefficients"},{"content":"Let $X=(X_1, X_2)$ and $Y=(Y_1, Y_2)$ be two random vectors in $\\operatorname{R}^2$. Suppose that $X_i$ and $Y_j$ are independent for each pair of indices. We have the following questions.\n Are $X$ and $Y$ independent? Are $X$ and $Y$ independent if each of them is bivariately normally distributed? Are $X$ and $Y$ independent if they are jointly normally distributed?  Answer Let $X_1, X_2 \\in U(0,1)$ be uniformly distributed and independent. Define the variables\n$$ \\begin{aligned} Y_1 \u0026amp;= X_1 + X_2 \\qquad \\operatorname{mod} 1 \\\\ Y_2 \u0026amp;= X_1 - X_2 \\qquad \\operatorname{mod} 1, \\end{aligned} $$\nwhere the $\\operatorname{mod} 1$ operation means that we identify all integer values on the real line and turn it into a circle of circumference one. Notice that $Y_1 \\mid X_1 \\sim U(0,1)$, hence the distribution of $Y_1$ does not depend on $X_1$. By the same token $Y_1$ is independent of $X_2$ as well. The argument also applies to $Y_2$, which is independent of $X_1$ and $X_2$. Hence all pairs of components are independent. However, the knowledge of $X$ fully determines the value of $Y$, hence $X$ and $Y$ are not independent.\nWe shall answer the second question by reducing it to the first one. To that end, let us consider the variables $X_1 \\in N(0,1)$ and $X_2 \\in N(0,1)$, $X_1$ and $X_2$ independent, and apply the transformation\n$$ \\begin{aligned} Z_1 \u0026amp;= F(X_1) + F(X_2) \\qquad \\operatorname{mod} 1 \\\\ Z_2 \u0026amp;= F(X_1) - F(X_2) \\qquad \\operatorname{mod} 1, \\end{aligned} $$\nwhere $F$ is the cumulative distribution function of the standard normal distribution. Since $F(X_i) \\in U(0,1)$, it follows that $Z_1$ and $Z_2$ have a standard uniform distribution. We now define the variables $Y_1$ and $Y_2$ such that\n$$ Y_1 | Z_1 \\sim N(Z_1,1), \\quad Y_2 | Z_2 \\sim N(Z_2,1). $$\nWe conclude that $X_i$ and $Y_j$ are independent for all pair of indices. However, the knowledge of $X$ fully determines the value of $Z$, which is a parameter in the distribution of $Y$. Hence $X$ and $Y$ are not independent.\nIf $X$ and $Y$ are jointly normally distributed, then pairwise independence does imply vector independence.\n","date":"2018-07-20","permalink":"/post/2018-07-20-vector-independence/","tags":["Probability"],"title":"Are two random vectors independent if their components are pairwise independent?"},{"content":"\u0026ldquo;A generating function is a device somewhat similar to a bag. Instead of carrying many little objects detachedly, which could be embarrassing, we put them all in a bag, and then we have only one object to carry, the bag.\u0026rdquo; \u0026ndash; George Polya, Mathematics and plausible reasoning (1954)\nIntroduction Often in mathematics we have to deal with recurrence relations. One of the best known examples of recurrence relations is the Fibonacci numbers given by the relation\n$$ {\\displaystyle F_{n}=F_{n-1}+F_{n-2}}, $$\nwith initial conditions ${\\displaystyle F_{0}=1}, {\\displaystyle F_{1}=1.}$ The Fibonacci sequence appeared in 1202 in the book *Liber Abaci* by the Italian mathematician Leonardo of Pisa, also known today as Fibonacci, where he tries to model the population growth of an idealized pair of newborn rabbits. The author supposes that in the beginning of each new month the number of rabbit pairs will be equal to their number $F_{n-1}$ a month ago plus the newborn pairs of rabbits, which would be equal to the number of rabbit pairs that were born two or more months ago, or $F_{n-2}$.\nWe present below an interesting and nontrivial mathematical problem about recurrence relations. The problem illustrates the rich mathematical theory of the subject and in particular the application of generating functions to study the solutions of recurrence relations.\nProblem Consider a random number sequence $S_n$ generated by the relation\n$$ \\begin{aligned} S_{n+1} = S_n + X, \\quad S_0 = 0, \\quad n = 0,1, \\dots, \\end{aligned} $$\nwhere $X$ is a random variable taking any value with equal probability from a given set of natural numbers $\\mathcal X = \\{n_1, n_2, \u0026hellip;, n_m\\}$. Let $p_k$ be the probability that the sequence $S_n$ will contain a given number $k$. Determine $p_k$ asymptotically in the form $p_k = \\pi + o(1)$, where $\\pi = \\pi(\\mathcal X)$ should be given explicitly.\nMotivational example In the simplest case where $\\mathcal X = \\{1,2\\}$, $p_n$ may be found explicitly to be\n$$ p_n = \\frac 2 3 + \\frac {(-1)^n }{2^{n} 3}, $$\nfor $n\\geq 1$, see below. Indeed, for $n=1$ the formula gives $p_1 = 1/2$, which is correct as we may reach $n=1$ starting from $n=0$ only if $X=1$. For $n=2$ the formula gives $p_2 = 3/4$. This is also correct as all possible pairs of values of $X$ are: $(1,1)$, $(1,2)$, $(2,1)$, and $(2,2)$. Of these, only the pair $(1,2)$ misses $n=2$ starting from $n=0$. Hence $n=2$ can be reached in 3 out of 4 ways, or $p_2=3/4$. We may continue computing $p_3$, $p_4$, $\\dots$ seeking for a pattern. And such a pattern readily emerges in the form of a recurrence relation between the probabilities $p_n$, $p_{n-1}$, $p_{n-2}$ given by\n$$ p_n = \\frac{p_{n-1} + p_{n-2}} 2. $$\nIndeed, the sequence may reach the number $n$ only if it undergoes the following two transitions with regard to its previous state: either from $n-1$ to $n$ or from $n-2$ to $n$. The first transition happens with probability $p_{n-1}/2$ and the second transition happens with probability $p_{n-2}/2$. The reader may notice that this recurrence relation looks similarly to the Fibonacci recurrence relation, however, the generated sequences have vastly different asymptotic properties.\nBefore we begin with the solution, we need to revise some basic facts.\nLinear recurrence relations with constant coefficients A linear recurrence relation is an equation of the form\n$$\\tag{1} x_n = c_1 x_{n-1} + c_2 x_{n-2} + \\cdots + c_k x_{n-k} $$\nthat defines the $n$-th term in a number sequence $x_n$ in terms of the $k$ previous terms in the sequence. The coefficients $c_i$ are all assumed to be constants. If $c_k\\not= 0$, the relation is said to be of order $k$. The recurrence relation here is called linear because all the terms in the sequence appear as first-order polynomials. This relation is also called homogenous because there is no constant term appearing in the equation. For example, the Fibonacci sequence is a linear homogeneous recurrence relation with constant coefficients of order 2. We briefly recall the main methods for solving linear recurrence relations with constant coefficients.\nCharacteristic polynomial The simplest recurrence relation is the first-order relation $x_n = r x_{n-1}$, $x_0=a$. Its solution is immediately seen to be $x_n=ar^n$. Looking for solutions of the general $k$-th order relation $(1)$ in the same form leads us to consider the roots of the polynomial\n$$\\tag{2} r^n - c_1 r^{n-1} - c_2 r^{n-2} - \\cdots - c_k r^{n-k}, $$\ncalled the characteristic polynomial of the recurrence relation $(1)$. The distinct roots $r_i$ of that polynomial induce solutions of the recurrence relation in the form $x_n = a_i r_i^n$, where $a_i$ are some constants. If $r$ is a repeated root of multiplicity $l$, then the solutions corresponding to that root have the general form\n$$ x_n = a_0 r^n + a_1 n r^n + \\cdots a_{l-1} n^{l-1} r^n, $$\nfor some constants $a_0$, $\\dots$, $a_{l-1}$. Since any polynomial of degree $k$ has $k$ roots counted with multiplicity, the roots of the characteristic polynomial always give us the full set of $k$ linearly independent solutions. The unspecified constants appearing in the general form of the solutions may be determined from the values of the first $k$ members of the recurrence relation.\nGenerating functions Sometimes finding the roots of the characteristic polynomial explicitly may not be feasible, especially if the polynomial is of degree higher than 2. Suppose, for example, that we want to show that a sequence $x_n$ generated by $(1)$ converges to some limit $L$ as $n\\rightarrow \\infty$. Such a limit would exist in the first place only if all the roots of $(2)$ are in absolute value less than one, possibly with the exception of one root of multiplicity one which may be equal to one. If this is the case, $x_n$ would be of the form $x_n = L + o(1)$, where by $o(1)$ we have denoted terms which tend to zero as $n\\rightarrow\\infty$. Determining $L$ from the initial conditions, however, might be a nontrivial task as obtaining all the roots of the characteristic polynomial explicitly cannot be done easily for degrees higher than two. In this case, the method of generating functions is indispensable.\nThe generating function of a number sequence $a_n$ is a function of the form\n$$ G(a_n;x)=\\sum_{n=0}^\\infty a_nx^n. $$\nIf $a_n=p_n$ is the probability mass function of a discrete random variable $X$ taking values in the non-negative integers ${0,1, \u0026hellip;}$, then its generating function is called the probability generating function of $X$.\nGenerating functions were first introduced by Abraham de Moivre in 1730 to study recurrence relations like $(1)$. Generating functions were extensively applied by Euler to study combinatorial and number theory problems in the 1750\u0026rsquo;s. The name generating function was coined by Laplace in connection to what is now known as the Laplace transform and the moment generating function of a random variable.\nExamples of generating functions for simple sequences Polynomials are generating functions to finite sequences.\nThe constant sequence 1,1,1,$\\dots$, is associated with the key generating function,\n$$ \\sum_{n=0}^\\infty x^n = \\frac 1 {1-x}, $$\nwhich is the sum of the geometric series with common ratio $x$.\nThe substitution $x \\rightarrow ax$ gives the generating function of the geometric sequence $1, a, a^2, a^3, \u0026hellip;$, for any constant $a$,\n$$ {\\displaystyle \\sum _{n=0}^{\\infty }(ax)^{n}={\\frac {1}{1-ax}}.} $$\nBy squaring the initial generating function, or by finding the derivative of both sides with respect to $x$ and making a change of running variable $n \\rightarrow n+1$, we get the generating function,\n$$ {\\displaystyle \\sum _{n=0}^{\\infty }(n+1)x^{n}={\\frac {1}{(1-x)^{2}}},} $$\nof the sequence $a_n = n+1$. More generally, for any non-negative integer $k$ and non-zero real value $a$, it is true that\n$$ {\\displaystyle \\sum _{n=0}^{\\infty }a^{n}{\\binom {n+k}{k}}x^{n}={\\frac {1}{(1-ax)^{k+1}}},.} $$\nNote that, since\n$$ {\\displaystyle 2{\\binom {n+2}{2}}-3{\\binom {n+1}{1}}+{\\binom {n}{0}}=2{\\frac {(n+1)(n+2)}{2}}-3(n+1)+1=n^{2},} $$\nwe may find the generating function for the sequence $a_n=n^2$ of square numbers as a sum of more elementary generating functions\n$$ {\\displaystyle {\\begin{aligned}G(n^{2};x)\u0026amp;=\\sum _{n=0}^{\\infty }n^{2}x^{n}={\\frac {2}{(1-x)^{3}}}-{\\frac {3}{(1-x)^{2}}}+{\\frac {1}{1-x}}={\\frac {x(x+1)}{(1-x)^{3}}}.\\end{aligned}}} $$\nBasic properties of generating functions The generating function of a number sequence can be expressed as a rational function (the ratio of two finite-degree polynomials) if and only if the sequence is generated by a linear recurrence relation with constant coefficients.\nFor the next property of generating functions let us recall the Cauchy product of two infinite sequences. If $a_i$ and $b_i$ are two infinite number sequences, the Cauchy product of the associated power series is defined as follows:\n$$ {\\displaystyle \\left(\\sum_{i=0}^{\\infty }a_{i}x^i\\right)\\cdot \\left(\\sum_{j=0}^{\\infty }b_{j}x^j\\right)=\\sum_{k=0}^{\\infty }c_{k}x^k} \\quad \\text{where} \\quad {\\displaystyle c_{k}=\\sum_{l=0}^{k}a_{l}b_{k-l}}. $$\nThe sequence $c_i$ is the called the Cauchy product of the sequences $a_i$ and $b_i$, or also the discrete convolution of $a_i$ and $b_i$. Thus, the product of two generating functions is the generating function of the Cauchy product of the associated sequences.\nFor example, the sequence of cumulative sums ${\\displaystyle (a_{0},a_{0}+a_{1},a_{0}+a_{1}+a_{2},\\cdots )}$ of a sequence with generating function $G(a_n; x)$ has the generating function\n$$ {\\displaystyle G(a_{n};x){\\frac {1}{1-x}}}. $$\nShifting sequence indices by $m$ steps to the right or $m$ steps to the left yields the following identities:\n$$ \\begin{aligned} x^{m}G(a_n; x) \u0026amp;= \\sum_{n\\geq m}a_{n-m}x^{n}=G(a_{n-m}; x) \\\\ \\frac { G(a_n;x)-a_{0}-a_{1}x-\\cdots -a_{m-1}x^{m-1}} {x^{m}} \u0026amp;= \\sum_{n\\geq 0}a_{n+m}x^{n}=G(a_{n+m}; x), \\end{aligned} $$\nwhere $a_{-1}, \\dots, a_{-m}$ are all set to zero.\nSolution of the problem First, without loss of generality we may assume that the numbers $n_1$, $\\dots$, $n_m$ are relative prime. Indeed, if $d \\geq 1$ is their greatest common divisor then $S_n$ will traverse only the numbers ${d, 2d, 3d, \\dots}$ as $S_n$ and $X$ will be divisible by $d$. Then we may write $X=dX'$, where $X'$ is a new random variable taking values in the set $\\mathcal X' = {n_1/d, n_2/d, \u0026hellip;, n_m/d}$. Then writing $S_n=dS_n'$, it is clearly enough to consider the problem only for the sequence $S_n'$.\nSuppose that the elements of the set $\\mathcal X$ are given in ascending order: $n_1 \\leq n_2 \\leq \\cdots \\leq n_m$. If $p_n$ is the probability the sequence $S_n$ to contain $n$ for any $n\\geq0$, then we have the recurrence relation\n$$ \\tag{3} p_{n} = \\frac {p_{n-n_1} + p_{n-n_2} + \\cdots + p_{n-n_m}} m, $$\nfor $n \\geq n_m$. The values of $p_0, p_1, \\dots, p_{n_m-1}$ represent the initial conditions of the recurrence relation, fully determining the corresponding sequence. These conditions are given by $p_0 = 1$ and by $(3)$ for $n = 1, \\dots, n_{m}-1$ by defining $p_n=0$, for $n\u0026lt;0$. If $g(x) = p_0 + p_1 x + p_2 x^2 + \\cdots$ is the generating function of the sequence $p_n$, then we have the identity\n$$ g(x) = \\frac{x^{n_1}g(x) + x^{n_{2}}g(x) + \\cdots + x^{n_m}g(x)} m +1. $$\nIndeed, this follows immediately from $(3)$ for all $p_n$ with $n \\geq 1$. Since $p_n$ does not satisfy $(3)$ only for $n=0$, we need to make a simple correction for it. Solving for $g$, we obtain\n$$\\tag{4} g(x) = \\frac {m} {m - (x^{n_{1}} + x^{n_{2}} \\cdots + x^{n_m})} = \\frac 1 {1-s(x)}, $$\nwhere apparently $s(x)=(x^{n_{1}} + x^{n_{2}} \\cdots + x^{n_m}) / m$ is the probability generating function of the random variable $X$.\nWe continue by noticing that $g(x)$ has a unique pole at $x=1$. If we can show that\n$$ g(x) = \\frac {C} {1 - x} + f(x), $$\nwhere $C$ is a suitable constant and $f(x)$ is analytic around $x=1$, then it would follow that $p_n = C + o(1)$ as $n\\rightarrow\\infty$. Indeed, the Taylor coefficients of analytic functions around $x=1$ must tend to zero as $n\\rightarrow\\infty$. Expanding the function $1-s(x)$ in a Taylor series around $x=1$, we obtain\n$$ 1 - s(x) = -\\mu(x-1) - \\mu (x-1)^2 r(x), $$\nwhere $\\mu = (n_1+n_2+\\cdots+n_m)/m = \\operatorname E X$ and $r(x)$ is some analytic function around $x=1$. It follows that\n$$ g(x) = \\frac 1 {\\mu} \\frac 1 {1 - x}\\frac 1 {1-(1-x)r(x)} = \\frac 1 {\\mu} \\frac 1 {1 - x}\\left(1+(1-x)r(x) + (1-x)^2r^2(x)+\\cdots\\right). $$\nThus, $g(x)$ has the desired form with $C=1/\\mu$ and $f(x) = (r(x) + (1-x)r^2(x) + \\cdots)/\\mu$. Hence, we conclude that\n$$ p_n = \\frac 1 {\\operatorname E X} + o(1)\\quad \\text{as} \\quad n\\rightarrow\\infty. $$\nExercise 1 To illustrate the method with the characteristic polynomial, let us consider the above problem in the particular case of $\\mathcal X=\\{1,2\\}$. Now the characteristic equation is given by\n$$ 2x^2 = x + 1. $$\nIts roots are $1$ and $-1/2$. We see that $p_n$ is in the form $\\displaystyle{p_n = 1^n a + (-1)^n{2^{-n}}b}$, where $a$ and $b$ are some constants to be determined. We have\n$$ \\begin{aligned} p_0 \u0026amp;= a + b= 1\\\\ p_1 \u0026amp;= a - b/2 = 1/2. \\end{aligned} $$\nFrom here $a=2/3$, $b=1/3$. Thus, we obtain\n$$ p_n = \\frac 2 3 + \\frac {(-1)^n }{2^{n} 3}, $$\nshowing that $p_n\\rightarrow 2/3$ as $n\\rightarrow\\infty$.\nExercise 2 We again consider the above problem in the particular case of $\\mathcal X=\\{1,2\\}$, but this time in terms of the generating function. In view of $(4)$, we now have $g(x) = {2} / (2-x-x^2)$. The roots of the polynomial $2-x-x^2$ are $1$ and $-2$. We write\n$$ g(x) = \\frac {1} {(1-x)(1+\\frac x 2)} = (1+x+x^2+\\cdots)(1-\\frac x 2 + \\frac {x^2} 4 - \\cdots). $$\nHence,\n$$ p_n = 1 - \\frac 1 2 + \\cdots + (-1)^n \\frac 1 {2^n} = \\frac 2 3(1 - (-1)^{n+1}\\frac 1 {2^{n+1}}) = \\frac 2 3 + \\frac {(-1)^n }{2^{n} 3}. $$\n","date":"2017-08-09","permalink":"/post/2017-08-09-generating-functions/","tags":["Probability"],"title":"Solving Recurrence Relations with Generating Functions"},{"content":"In this tutorial, we are going to show how one can gather data about the esports game Dota 2 from a dedicated site collecting and providing such data. The data will be requested through the web site\u0026rsquo;s API interface documented here: The Open Dota API Documentation. First, we\u0026rsquo;re going to cover the basics of accessing an API using the R programming language.\nAPIs allow programmers to request data directly from certain websites through what\u0026rsquo;s called an Application Programming Interface. When a website sets up an API, they are essentially setting up a computer that waits for data requests. Once this computer receives a data request, it will do its own processing of the data and send it to the computer that requested it. From our perspective as the requester, we will need to write code in R that creates the request and tells the computer running the API what we need. That computer will then read our code, process the request, and return nicely-formatted data that can be easily parsed by existing R libraries.\nMaking API requests in R To work with APIs in R, we need to bring in some libraries. These libraries take all of the complexities of an API request and wrap them up in functions that we can use in single lines of code. The R libraries that we’ll be using are httr and jsonlite. If you don’t have either of these libraries in your R console or RStudio, you’ll need to download them first. Use the install.packages() function to bring in these packages.\ninstall.packages(c(\u0026quot;httr\u0026quot;, \u0026quot;jsonlite\u0026quot;))\r After downloading the libraries, we’ll be able to use them in our R scripts or RMarkdown files.\nlibrary(httr)\rlibrary(jsonlite)\r For our purposes, we’ll just be asking for data, which corresponds to a GET request. In order to create a GET request, we need to use the GET() function from the httr library. The GET() function requires a URL, which specifies the address of the server that the request needs to be sent to. For example, the full list with such URL addresses supported by Open Dota is given in their documentation. Let us make a request for professional Dota 2 matches.\npro_matches_raw = GET(\u0026quot;https://api.opendota.com/api/proMatches\u0026quot;)\r Investigating the pro_matches_raw variable gives us a summary look at the resulting response. The first thing to notice is that it contains the URL that the GET request was sent to. We can also see the date and time that the request was made, as well as the size of the response. The content type gives us an idea of what form the data takes. This particular response says that the data takes on a json format, which gives a hint about why we need the jsonlite library.\npro_matches_raw\r ## Response [https://api.opendota.com/api/proMatches]\r## Date: 2021-07-26 09:36\r## Status: 200\r## Content-Type: application/json; charset=utf-8\r## Size: 32 kB\r Handling JSON Data JSON stands for JavaScript Object Notation. While JavaScript is another programming language, our focus on JSON is its structure. JSON is useful because it is easily readable by a computer, and for this reason, it has become the primary way that data is transported through APIs. Most APIs will send their responses in JSON format.\nThe rawToChar() is an R base function that converts the unicode content of the request into JSON format. Then we use the fromJSON() function from jasonlite to convert the JSON code into a data.frame structure.\npro_matches = fromJSON(rawToChar(pro_matches_raw$content))\rpro_matches %\u0026gt;% head %\u0026gt;% kbl %\u0026gt;% kable_paper() %\u0026gt;% scroll_box(width = \u0026quot;100%\u0026quot;, height = \u0026quot;250px\u0026quot;)\r \r\rmatch_id \rduration \rstart_time \rradiant_team_id \rradiant_name \rdire_team_id \rdire_name \rleagueid \rleague_name \rseries_id \rseries_type \rradiant_score \rdire_score \rradiant_win \r\r\r\r\r6106012716 \r1163 \r1627287926 \r8449479 \rTeam GL \r5184391 \rEYE gaming \r13268 \rUltras Dota Pro \r581335 \r1 \r15 \r34 \rFALSE \r\r\r6106012486 \r1459 \r1627287956 \r8488438 \rNA \r8488435 \rNA \r13395 \rEast Power Cup \r581344 \r0 \r7 \r34 \rFALSE \r\r\r6106009678 \r3137 \r1627287781 \r8488435 \rNA \r8488432 \rNA \r13395 \rEast Power Cup \r581343 \r0 \r0 \r0 \rFALSE \r\r\r6106005670 \r3129 \r1627287539 \r8488432 \rNA \r8488435 \rNA \r13395 \rEast Power Cup \r581341 \r2 \r0 \r0 \rTRUE \r\r\r6106004860 \r1634 \r1627287492 \r8400307 \rits a PRANK \r8336189 \rDota Geniuses \r13286 \rEfusion Dota 2 League \r581340 \r1 \r40 \r18 \rTRUE \r\r\r6105994354 \r2293 \r1627286827 \r8254145 \rExecration \r8482620 \rNA \r13335 \rPerfect Land Gaming \r581334 \r1 \r25 \r49 \rFALSE \r\r\r\rUnderstanding the Dota 2 variables The match_id, radiant_team_id and dota_team_id are identifiers for the match, and the radiant and dire teams, respectively. The start_time gives the date and time of the match stored as Unix time:\npro_matches$start_time[1] %\u0026gt;% as.POSIXct(origin = \u0026quot;1970-01-01\u0026quot;)\r ## [1] \u0026quot;2021-07-26 11:25:26 EEST\u0026quot;\r The professional Dota 2 games are held in series of tournaments organized by Dota 2 leagues. The leagueid and league_name specify a Dota 2 league. The radiant_score and dire_score give the number of heroes killed by the opposite team.\nGetting match details To get more details about a Dota 2 match such the names of the 10 players and in-game statistics, we use the API call\nhttps://api.opendota.com/api/matches/{match_id}.\nWe supply the identifier of the match we are interested in to the field {match_id}. Let us for example, consider the match id of the first result.\nmatch_id = pro_matches$match_id[1]\rbase_url = \u0026quot;https://api.opendota.com/api/matches/\u0026quot;\rapi_call = paste0(base_url,match_id)\rapi_call\r ## [1] \u0026quot;https://api.opendota.com/api/matches/6106012716\u0026quot;\r The following request\nmatch_json = rawToChar(GET(api_call)$content)\rm = fromJSON(match_json)\r returns a long list of nested lists, taking lots of memory:\nobject.size(m)\r ## 882040 bytes\r Let us see the names of the top-level list elements.\nnames(m)\r ## [1] \u0026quot;match_id\u0026quot; \u0026quot;barracks_status_dire\u0026quot; \u0026quot;barracks_status_radiant\u0026quot;\r## [4] \u0026quot;chat\u0026quot; \u0026quot;cluster\u0026quot; \u0026quot;cosmetics\u0026quot; ## [7] \u0026quot;dire_score\u0026quot; \u0026quot;dire_team_id\u0026quot; \u0026quot;draft_timings\u0026quot; ## [10] \u0026quot;duration\u0026quot; \u0026quot;engine\u0026quot; \u0026quot;first_blood_time\u0026quot; ## [13] \u0026quot;game_mode\u0026quot; \u0026quot;human_players\u0026quot; \u0026quot;leagueid\u0026quot; ## [16] \u0026quot;lobby_type\u0026quot; \u0026quot;match_seq_num\u0026quot; \u0026quot;negative_votes\u0026quot; ## [19] \u0026quot;objectives\u0026quot; \u0026quot;picks_bans\u0026quot; \u0026quot;positive_votes\u0026quot; ## [22] \u0026quot;radiant_gold_adv\u0026quot; \u0026quot;radiant_score\u0026quot; \u0026quot;radiant_team_id\u0026quot; ## [25] \u0026quot;radiant_win\u0026quot; \u0026quot;radiant_xp_adv\u0026quot; \u0026quot;skill\u0026quot; ## [28] \u0026quot;start_time\u0026quot; \u0026quot;teamfights\u0026quot; \u0026quot;tower_status_dire\u0026quot; ## [31] \u0026quot;tower_status_radiant\u0026quot; \u0026quot;version\u0026quot; \u0026quot;replay_salt\u0026quot; ## [34] \u0026quot;series_id\u0026quot; \u0026quot;series_type\u0026quot; \u0026quot;league\u0026quot; ## [37] \u0026quot;radiant_team\u0026quot; \u0026quot;dire_team\u0026quot; \u0026quot;players\u0026quot; ## [40] \u0026quot;patch\u0026quot; \u0026quot;region\u0026quot; \u0026quot;all_word_counts\u0026quot; ## [43] \u0026quot;my_word_counts\u0026quot; \u0026quot;comeback\u0026quot; \u0026quot;stomp\u0026quot; ## [46] \u0026quot;replay_url\u0026quot;\r The list elements radiant_gold_adv1 and radiant_xp_adv are time series values giving as the difference in gold and experience between the two teams each minute of the game.\nlibrary(data.table)\rd = data.table(minute = 1:length(m$radiant_gold_adv), gold = m$radiant_gold_adv, xp = m$radiant_xp_adv)\rd = melt(d, id.vars = 'minute')\rggplot2::qplot(minute, value, data = d, color = variable, geom = c('point', 'line'), main = paste('Match id = ', match_id))  The players list element is a large list with details about the 10 human players.\nnames(m$players)\r ## [1] \u0026quot;match_id\u0026quot; \u0026quot;player_slot\u0026quot; \u0026quot;ability_targets\u0026quot; ## [4] \u0026quot;ability_upgrades_arr\u0026quot; \u0026quot;ability_uses\u0026quot; \u0026quot;account_id\u0026quot; ## [7] \u0026quot;actions\u0026quot; \u0026quot;additional_units\u0026quot; \u0026quot;assists\u0026quot; ## [10] \u0026quot;backpack_0\u0026quot; \u0026quot;backpack_1\u0026quot; \u0026quot;backpack_2\u0026quot; ## [13] \u0026quot;backpack_3\u0026quot; \u0026quot;buyback_log\u0026quot; \u0026quot;camps_stacked\u0026quot; ## [16] \u0026quot;connection_log\u0026quot; \u0026quot;creeps_stacked\u0026quot; \u0026quot;damage\u0026quot; ## [19] \u0026quot;damage_inflictor\u0026quot; \u0026quot;damage_inflictor_received\u0026quot; \u0026quot;damage_taken\u0026quot; ## [22] \u0026quot;damage_targets\u0026quot; \u0026quot;deaths\u0026quot; \u0026quot;denies\u0026quot; ## [25] \u0026quot;dn_t\u0026quot; \u0026quot;firstblood_claimed\u0026quot; \u0026quot;gold\u0026quot; ## [28] \u0026quot;gold_per_min\u0026quot; \u0026quot;gold_reasons\u0026quot; \u0026quot;gold_spent\u0026quot; ## [31] \u0026quot;gold_t\u0026quot; \u0026quot;hero_damage\u0026quot; \u0026quot;hero_healing\u0026quot; ## [34] \u0026quot;hero_hits\u0026quot; \u0026quot;hero_id\u0026quot; \u0026quot;item_0\u0026quot; ## [37] \u0026quot;item_1\u0026quot; \u0026quot;item_2\u0026quot; \u0026quot;item_3\u0026quot; ## [40] \u0026quot;item_4\u0026quot; \u0026quot;item_5\u0026quot; \u0026quot;item_neutral\u0026quot; ## [43] \u0026quot;item_uses\u0026quot; \u0026quot;kill_streaks\u0026quot; \u0026quot;killed\u0026quot; ## [46] \u0026quot;killed_by\u0026quot; \u0026quot;kills\u0026quot; \u0026quot;kills_log\u0026quot; ## [49] \u0026quot;lane_pos\u0026quot; \u0026quot;last_hits\u0026quot; \u0026quot;leaver_status\u0026quot; ## [52] \u0026quot;level\u0026quot; \u0026quot;lh_t\u0026quot; \u0026quot;life_state\u0026quot; ## [55] \u0026quot;max_hero_hit\u0026quot; \u0026quot;multi_kills\u0026quot; \u0026quot;net_worth\u0026quot; ## [58] \u0026quot;obs\u0026quot; \u0026quot;obs_left_log\u0026quot; \u0026quot;obs_log\u0026quot; ## [61] \u0026quot;obs_placed\u0026quot; \u0026quot;party_id\u0026quot; \u0026quot;party_size\u0026quot; ## [64] \u0026quot;performance_others\u0026quot; \u0026quot;permanent_buffs\u0026quot; \u0026quot;pings\u0026quot; ## [67] \u0026quot;pred_vict\u0026quot; \u0026quot;purchase\u0026quot; \u0026quot;purchase_log\u0026quot; ## [70] \u0026quot;randomed\u0026quot; \u0026quot;repicked\u0026quot; \u0026quot;roshans_killed\u0026quot; ## [73] \u0026quot;rune_pickups\u0026quot; \u0026quot;runes\u0026quot; \u0026quot;runes_log\u0026quot; ## [76] \u0026quot;sen\u0026quot; \u0026quot;sen_left_log\u0026quot; \u0026quot;sen_log\u0026quot; ## [79] \u0026quot;sen_placed\u0026quot; \u0026quot;stuns\u0026quot; \u0026quot;teamfight_participation\u0026quot; ## [82] \u0026quot;times\u0026quot; \u0026quot;tower_damage\u0026quot; \u0026quot;towers_killed\u0026quot; ## [85] \u0026quot;xp_per_min\u0026quot; \u0026quot;xp_reasons\u0026quot; \u0026quot;xp_t\u0026quot; ## [88] \u0026quot;personaname\u0026quot; \u0026quot;name\u0026quot; \u0026quot;last_login\u0026quot; ## [91] \u0026quot;radiant_win\u0026quot; \u0026quot;start_time\u0026quot; \u0026quot;duration\u0026quot; ## [94] \u0026quot;cluster\u0026quot; \u0026quot;lobby_type\u0026quot; \u0026quot;game_mode\u0026quot; ## [97] \u0026quot;is_contributor\u0026quot; \u0026quot;patch\u0026quot; \u0026quot;region\u0026quot; ## [100] \u0026quot;isRadiant\u0026quot; \u0026quot;win\u0026quot; \u0026quot;lose\u0026quot; ## [103] \u0026quot;total_gold\u0026quot; \u0026quot;total_xp\u0026quot; \u0026quot;kills_per_min\u0026quot; ## [106] \u0026quot;kda\u0026quot; \u0026quot;abandons\u0026quot; \u0026quot;neutral_kills\u0026quot; ## [109] \u0026quot;tower_kills\u0026quot; \u0026quot;courier_kills\u0026quot; \u0026quot;lane_kills\u0026quot; ## [112] \u0026quot;hero_kills\u0026quot; \u0026quot;observer_kills\u0026quot; \u0026quot;sentry_kills\u0026quot; ## [115] \u0026quot;roshan_kills\u0026quot; \u0026quot;necronomicon_kills\u0026quot; \u0026quot;ancient_kills\u0026quot; ## [118] \u0026quot;buyback_count\u0026quot; \u0026quot;observer_uses\u0026quot; \u0026quot;sentry_uses\u0026quot; ## [121] \u0026quot;lane_efficiency\u0026quot; \u0026quot;lane_efficiency_pct\u0026quot; \u0026quot;lane\u0026quot; ## [124] \u0026quot;lane_role\u0026quot; \u0026quot;is_roaming\u0026quot; \u0026quot;purchase_time\u0026quot; ## [127] \u0026quot;first_purchase_time\u0026quot; \u0026quot;item_win\u0026quot; \u0026quot;item_usage\u0026quot; ## [130] \u0026quot;purchase_ward_observer\u0026quot; \u0026quot;actions_per_min\u0026quot; \u0026quot;life_state_dead\u0026quot; ## [133] \u0026quot;rank_tier\u0026quot; \u0026quot;cosmetics\u0026quot; \u0026quot;benchmarks\u0026quot; ## [136] \u0026quot;purchase_ward_sentry\u0026quot; \u0026quot;purchase_tpscroll\u0026quot;\r These are some basics of the Open Dota API. For more information read the documentation or visit the dotabuff site.\n","date":"0001-01-01","permalink":"/post/open_dota_api/","tags":null,"title":"The Open Dota API"}]